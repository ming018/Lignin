import os
import csv
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
import seaborn as sns

from stable_baselines3 import PPO, A2C, DQN
from models.ByproductPredictorCNN import ByproductPredictorCNN
from models.TemperatureToDataPredictorCNN import TemperatureToDataPredictorCNN
from models.TemperatureToCompositionPredictor import TemperatureToCompositionPredictor
from TemperatureEnv import TemperatureEnv

# ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
reward_modes = ["biofuel", "high_value", "functional"]
algorithms = {"PPO": PPO, "A2C": A2C, "DQN": DQN}
total_timesteps = 10000
log_dir = "logs_rl_compare_all"
os.makedirs(log_dir, exist_ok=True)

# ëª¨ë¸ ë¡œë“œ
TGA_model = ByproductPredictorCNN(1, 761).to(device)
TGA_model.load_state_dict(torch.load('pth/tga.pth', map_location=device))
TGA_model.eval()

FTIR_model = TemperatureToDataPredictorCNN(input_size=1).to(device)
FTIR_model.load_state_dict(torch.load('pth/FTIR_model.pth', map_location=device))
FTIR_model.eval()

GCMS_model = TemperatureToCompositionPredictor(input_size=1, output_size=10).to(device)
GCMS_model.load_state_dict(torch.load('pth/composition_model.pth', map_location=device))
GCMS_model.eval()

all_results = []

# ì£¼ ëª©ì ë³„ë¡œ í•™ìŠµ ë° ë¡œê·¸ ìˆ˜ì§‘
for reward_mode in reward_modes:
    for algo_name, AlgoClass in algorithms.items():
        print(f"â–¶ ì‹œì‘: {reward_mode} - {algo_name}")
        env = TemperatureEnv(TGA_model, FTIR_model, GCMS_model, device, reward_mode=reward_mode)
        model = AlgoClass("MlpPolicy", env, verbose=0)
        model.learn(total_timesteps=total_timesteps)
        print(f"âœ” í•™ìŠµ ì™„ë£Œ: {reward_mode} - {algo_name}")

        obs, _ = env.reset()
        log_path = os.path.join(log_dir, f"log_{reward_mode}_{algo_name}.csv")
        with open(log_path, 'w', newline='') as f:
            writer = csv.writer(f)
            header = ['step', 'temperature', 'reward'] + [f'GCMS_{i}' for i in range(10)]
            writer.writerow(header)

        best_reward = -float('inf')
        best_log = None

        for step in range(50):
            action, _ = model.predict(obs)
            obs, reward, terminated, truncated, info = env.step(action)
            row = [step, obs[0], reward] + list(np.round(info['gcms'], 4))

            with open(log_path, 'a', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(row)

            if reward > best_reward:
                best_reward = reward
                best_log = row

            if terminated or truncated:
                break

        model.save(os.path.join(log_dir, f"{algo_name}_{reward_mode}"))
        if best_log is not None and len(best_log) == 13:
            all_results.append((reward_mode, algo_name, best_log[1], best_log[2], *best_log[3:]))
        else:
            print(f"[ê²½ê³ ] {algo_name}-{reward_mode} ë¡œê·¸ ëˆ„ë½ ë˜ëŠ” ì†ìƒ")

        print(f"âœ… ì™„ë£Œ: {reward_mode} - {algo_name}")

# ì‹œê°í™” ê²°ê³¼ ì €ì¥
summary_df = pd.DataFrame(all_results, columns=["Task", "Algorithm", "Best Temperature", "Best Reward"] + [f'GCMS_{i}' for i in range(10)])
summary_path = os.path.join(log_dir, "summary.csv")
summary_df.to_csv(summary_path, index=False)

# í…ìŠ¤íŠ¸ ì¶œë ¥
print("ğŸ“Š GCMS ìµœì  ì¡°ì„± ìš”ì•½:")
print(summary_df.to_string(index=False))

# GCMS í•­ëª©ë³„ ì˜í–¥ë„ ì‹œê°í™”
summary_long = summary_df.melt(id_vars=["Task", "Algorithm", "Best Temperature", "Best Reward"],
                                value_vars=[f'GCMS_{i}' for i in range(10)],
                                var_name="GCMS_Component", value_name="Composition")

plt.figure(figsize=(12, 6))
sns.barplot(data=summary_long, x="GCMS_Component", y="Composition", hue="Task")
plt.title("ê° ëª©ì ë³„ ìµœì  ì˜¨ë„ì—ì„œì˜ GCMS ì¡°ì„±")
plt.ylabel("Composition (%)")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()